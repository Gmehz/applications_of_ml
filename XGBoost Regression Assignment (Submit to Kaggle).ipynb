{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by: \n",
    "\n",
    "- https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression\n",
    "- https://www.kaggle.com/code/dansbecker/xgboost/notebook\n",
    "\n",
    "In this assignment we will apply XGBoost Regression techniques to predict house prices, based on the famous Kaggle Dataset https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n",
    "\n",
    "Step 1 is to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# load up the libraries\n",
    "#=========================================================================\n",
    "import pandas  as pd\n",
    "import numpy   as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "#=========================================================================\n",
    "# read in the data\n",
    "#=========================================================================\n",
    "train_data = pd.read_csv('train.csv',index_col=0)\n",
    "test_data  = pd.read_csv('test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 80)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature selection</center>\n",
    "The purpose of feature selection, as the name suggests, is to only model the most pertinent and important features, thus reducing the computational overhead, and also to alleviate the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). The following are a number of notebooks covering techniques to achieve said goal, all of which use the House Prices data as an example:\n",
    "\n",
    "* [Feature selection using the Boruta-SHAP package](https://www.kaggle.com/carlmcbrideellis/feature-selection-using-the-boruta-shap-package)\n",
    "* [Recursive Feature Elimination (RFE) example](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example)\n",
    "* [House Prices: Permutation Importance example](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example)\n",
    "* [Feature importance using the LASSO](https://www.kaggle.com/carlmcbrideellis/feature-importance-using-the-lasso)\n",
    "\n",
    "In this assignment, we shall use all of the numerical columns, and ignore the categorical features. To encode the categorical features one can use for example [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html). \n",
    "\n",
    "Our first task is to do Feature Exploration and Selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use only numerical predictors\n",
    "train_predictors = train_data.select_dtypes(exclude=['object'])\n",
    "test_predictors = test_data.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Impute missing values with mean\n",
    "train_predictors.fillna(train_predictors.mean(), inplace=True)\n",
    "test_predictors.fillna(test_predictors.mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature engineering</center>\n",
    "As mentioned, one aspect of feature engineering is the creation of new features out of existing features. A simple example would be to create a new feature which is the sum of the number of bathrooms in the house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Preparing the data for modeling\n",
    "# Select the target variable\n",
    "y = train_data['SalePrice']\n",
    "\n",
    "# Split the data into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_predictors, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y: (1460,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1168, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (292, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the number of bathrooms\n",
    "#X_train['n_bathrooms'] = X_train['BsmtFullBath'] + (0.5 * X_train['BsmtHalfBath']) + X_train['FullBath'] + (0.5 * X_train['HalfBath'])\n",
    "#X_test['n_bathrooms'] = X_test['BsmtFullBath'] + (0.5 * X_test['BsmtHalfBath']) + X_test['FullBath'] + (0.5 * X_test['HalfBath'])\n",
    "\n",
    "# Calculate the area including basement\n",
    "#X_train['area_with_basement'] = X_train['GrLivArea'] + X_train['TotalBsmtSF']\n",
    "#X_test['area_with_basement'] = X_test['GrLivArea'] + X_test['TotalBsmtSF']\n",
    "\n",
    "# Step 5: Feature engineering\n",
    "# Calculate the number of bathrooms\n",
    "for df in [X_train, X_test]:\n",
    "    df['n_bathrooms'] = df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']) + df['FullBath'] + (0.5 * df['HalfBath'])\n",
    "\n",
    "# Calculate the area including basement\n",
    "for df in [X_train, X_test]:\n",
    "    df['area_with_basement'] = df['GrLivArea'] + df['TotalBsmtSF']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to apply some feature engineering to prepare for using the XGBoost Estimator to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate Total Square Foot of the house\n",
    "\n",
    "for df in [train_data, test_data]:\n",
    "    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "\n",
    "for df in [train_data, test_data]:\n",
    "    df['Age'] = df['YrSold'] - df['YearBuilt']\n",
    "\n",
    "#X_train['TotalSF'] = X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\n",
    "#X_test['TotalSF'] = X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']\n",
    "\n",
    "# Calculate Age of the house at the time of sale\n",
    "#X_train['Age'] = X_train['YrSold'] - X_train['YearBuilt']\n",
    "#X_test['Age'] = X_test['YrSold'] - X_test['YearBuilt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1168, 42)\n"
     ]
    }
   ],
   "source": [
    "print( 'X_train shape:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (292, 42)\n"
     ]
    }
   ],
   "source": [
    "print( 'X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train: (1168,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_train:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more on this fascinating aspect may I recommend the free on-line book [\"*Feature Engineering and Selection: A Practical Approach for Predictive Models*\"](http://www.feat.engineering/) by Max Kuhn and Kjell Johnson.\n",
    "### <center style=\"background-color:Gainsboro; width:60%;\">XGBoost estimator</center>\n",
    "Note that for this competition we use the RMSLE evaluation metric, rather than the default metric, which for regression is the RMSE. For more on the peculiarities of the RMSLE see the Appendix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:04.016433Z",
     "iopub.status.busy": "2022-10-17T08:49:04.015704Z",
     "iopub.status.idle": "2022-10-17T08:50:04.782101Z",
     "shell.execute_reply": "2022-10-17T08:50:04.781125Z",
     "shell.execute_reply.started": "2022-10-17T08:49:04.016384Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# XGBoost regression: \n",
    "# Parameters: \n",
    "# n_estimators  \"Number of gradient boosted trees. Equivalent to number \n",
    "#                of boosting rounds.\"\n",
    "# learning_rate \"Boosting learning rate (also known as “eta”)\"\n",
    "# max_depth     \"Maximum depth of a tree. Increasing this value will make \n",
    "#                the model more complex and more likely to overfit.\" \n",
    "#=========================================================================\n",
    "#regressor=xgb.XGBRegressor(eval_metric='rmsle')\n",
    "\n",
    "#=========================================================================\n",
    "# exhaustively search for the optimal hyperparameters\n",
    "#=========================================================================\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "# set up our search grid\n",
    "#param_grid = {\"max_depth\":    [4, 5],\n",
    "              #\"n_estimators\": [500, 600, 700],\n",
    "              #\"learning_rate\": [0.01, 0.015]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use grid search to find the optimal hyper parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset used for fitting the model: (1168, 42) (1168,)\n"
     ]
    }
   ],
   "source": [
    "# Inside the GridSearchCV code (before fitting the model)\n",
    "print(\"Shape of dataset used for fitting the model:\", X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=500; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=700; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=4, n_estimators=700; total time=   1.8s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=500; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=500; total time=   1.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=500; total time=   1.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=500; total time=   1.3s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=500; total time=   1.1s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=600; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=600; total time=   1.5s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=700; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=700; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=700; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=700; total time=   1.6s\n",
      "[CV] END ..learning_rate=0.01, max_depth=5, n_estimators=700; total time=   1.7s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=500; total time=   1.1s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=500; total time=   1.0s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=600; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=700; total time=   1.4s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=700; total time=   1.7s\n",
      "[CV] END .learning_rate=0.015, max_depth=4, n_estimators=700; total time=   1.5s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=500; total time=   1.1s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=500; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=500; total time=   1.5s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=500; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=500; total time=   1.2s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=600; total time=   1.5s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=600; total time=   1.5s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=600; total time=   1.6s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=600; total time=   1.4s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=700; total time=   1.7s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=700; total time=   1.8s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=700; total time=   1.8s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=700; total time=   1.9s\n",
      "[CV] END .learning_rate=0.015, max_depth=5, n_estimators=700; total time=   1.9s\n",
      "Best parameters found:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 600}\n",
      "Best score found:  0.9959983929747972\n"
     ]
    }
   ],
   "source": [
    "# Use GridSearchCV to find the best parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define our XGBoost regressor\n",
    "xgb_regressor = xgb.XGBRegressor(eval_metric='rmsle')\n",
    "\n",
    "# Set up our search grid\n",
    "param_grid = {\"max_depth\": [4, 5],\n",
    "              \"n_estimators\": [500, 600, 700],\n",
    "              \"learning_rate\": [0.01, 0.015]}\n",
    "\n",
    "# Define GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_regressor,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "# Run the Grid Search\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 600}\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score found:  0.9959983929747972\n"
     ]
    }
   ],
   "source": [
    "# Print the best score found\n",
    "print(\"Best score found: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you setup a XGBoost Regressor object using your hyperparameters and fit it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=&#x27;rmsle&#x27;, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=&#x27;rmsle&#x27;, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric='rmsle', feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=4, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the XGBoost regressor with the optimal hyperparameters\n",
    "optimal_xgb_regressor = xgb.XGBRegressor(learning_rate=0.01, max_depth=4, n_estimators=600, eval_metric='rmsle')\n",
    "\n",
    "# Fit the model to the training data\n",
    "optimal_xgb_regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on validation data:  0.017178577380050315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = optimal_xgb_regressor.predict(X_test)\n",
    "\n",
    "# Compute RMSLE on the validation data\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "\n",
    "print(\"RMSLE on validation data: \", rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, can you run it on your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on validation data:  0.017178577380050315\n"
     ]
    }
   ],
   "source": [
    "# Run the model on the test.csv*****\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "# Train the XGBoost model with the optimal hyperparameters\n",
    "optimal_xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = optimal_xgb_regressor.predict(X_test)\n",
    "\n",
    "# Compute RMSLE on the validation data\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "\n",
    "print(\"RMSLE on validation data: \", rmsle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X_train:  Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold', 'SalePrice', 'TotalSF', 'Age', 'HasGarage',\n",
      "       'n_bathrooms', 'area_with_basement'],\n",
      "      dtype='object')\n",
      "Columns in test_predictors:  Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold', 'TotalSF', 'Age', 'HasGarage'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names in X_train and test_predictors\n",
    "print(\"Columns in X_train: \", X_train.columns)\n",
    "print(\"Columns in test_predictors: \", test_predictors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the missing columns in test_predictors\n",
    "missing_columns = set(X_train.columns) - set(test_predictors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the missing columns to test_predictors with default values (e.g., 0)\n",
    "for col in missing_columns:\n",
    "    test_predictors[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reorder test_predictors columns to match X_train\n",
    "test_predictors = test_predictors[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted house prices for test data:  [39016.98 39016.98 39016.98 ... 39016.98 39016.98 39016.98]\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on test data\n",
    "test_pred = optimal_xgb_regressor.predict(test_predictors)\n",
    "\n",
    "# Print the predicted house prices for the test data\n",
    "print(\"Predicted house prices for test data: \", test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Can you score your solution offline and see how it does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSLE on test data:  0.017178577380050315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "from math import sqrt\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_pred = optimal_xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the RMSLE\n",
    "rmsle = sqrt(mean_squared_log_error(y_test, test_pred))\n",
    "\n",
    "print(\"RMSLE on test data: \", rmsle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1459, 292]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m optimal_xgb_regressor\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate the RMSLE\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m RMSLE \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_log_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score is \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m RMSLE)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:561\u001b[0m, in \u001b[0;36mmean_squared_log_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    494\u001b[0m     {\n\u001b[0;32m    495\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    505\u001b[0m ):\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;124;03m\"\"\"Mean squared logarithmic error regression loss.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \n\u001b[0;32m    508\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_log_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m    0.060...\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (y_true \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m (y_pred \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_regression.py:99\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1459, 292]"
     ]
    }
   ],
   "source": [
    "# read in the ground truth file\n",
    "solution = pd.read_csv('sample_submission.csv') \n",
    "y_true = solution[\"SalePrice\"]\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = optimal_xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate the RMSLE\n",
    "RMSLE = np.sqrt(mean_squared_log_error(y_true, predictions))\n",
    "print(\"The score is %.5f\" % RMSLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the below block to prepare your submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.964744Z",
     "iopub.status.busy": "2022-10-17T08:50:06.964340Z",
     "iopub.status.idle": "2022-10-17T08:50:06.977852Z",
     "shell.execute_reply": "2022-10-17T08:50:06.976712Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.964712Z"
    }
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({\"Id\":test_data.index, \"SalePrice\":predictions})\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Feature importance</center>\n",
    "Let us also take a very quick look at the feature importance too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:06.980369Z",
     "iopub.status.busy": "2022-10-17T08:50:06.979530Z",
     "iopub.status.idle": "2022-10-17T08:50:07.282871Z",
     "shell.execute_reply": "2022-10-17T08:50:07.281929Z",
     "shell.execute_reply.started": "2022-10-17T08:50:06.980309Z"
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "plot_importance(regressor, max_num_features=8, ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where here the `F score` is a measure \"*...based on the number of times a variable is selected for splitting, weighted by the squared improvement to the model as a result of each split, and averaged over all trees*.\" [1] \n",
    "\n",
    "Note that these importances are susceptible to small changes in the training data, and it is much better to make use of [\"GPU accelerated SHAP values\"](https://www.kaggle.com/carlmcbrideellis/gpu-accelerated-shap-values-jane-street-example), incorporated with version 1.3 of XGBoost.\n",
    "\n",
    "Can you follow the above guide use SHAP values instead of F Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center style=\"background-color:Gainsboro; width:60%;\">Appendix: The RMSLE evaluation metric</center>\n",
    "From the competition [evaluation page](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) we see that the metric we are using is the root mean squared logarithmic error (RMSLE), which is given by\n",
    "\n",
    "$$ {\\mathrm {RMSLE}}\\,(y, \\hat y) = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n \\left(\\log (1 + \\hat{y}_i) - \\log (1 + y_i)\\right)^2} $$\n",
    "\n",
    "where $\\hat{y}_i$ is the predicted value of the target for instance $i$, and $y_i$\n",
    "is the actual value of the target for instance $i$.\n",
    "\n",
    "It is important to note that, unlike the RMSE, the RMSLE is asymmetric; penalizing much more the underestimated predictions than the overestimated predictions. For example, say the correct value is $y_i = 1000$, then underestimating by 600 is almost twice as bad as overestimating by 600:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.284616Z",
     "iopub.status.busy": "2022-10-17T08:50:07.284340Z",
     "iopub.status.idle": "2022-10-17T08:50:07.290854Z",
     "shell.execute_reply": "2022-10-17T08:50:07.289987Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.284584Z"
    }
   },
   "outputs": [],
   "source": [
    "def RSLE(y_hat,y):\n",
    "    return np.sqrt((np.log1p(y_hat) - np.log1p(y))**2)\n",
    "\n",
    "print(\"The RMSLE score is %.3f\" % RSLE( 400,1000) )\n",
    "print(\"The RMSLE score is %.3f\" % RSLE(1600,1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymmetry arises because \n",
    "\n",
    "$$ \\log (1 + \\hat{y}_i) - \\log (1 + y_i) =  \\log \\left( \\frac{1 + \\hat{y}_i}{1 + y_i} \\right) $$\n",
    "\n",
    "so we are essentially looking at ratios, rather than differences such as is the case of the RMSE. We can see the form that this asymmetry takes in the following plot, again using 1000 as our ground truth value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-10-17T08:50:07.292446Z",
     "iopub.status.busy": "2022-10-17T08:50:07.292191Z",
     "iopub.status.idle": "2022-10-17T08:50:07.467273Z",
     "shell.execute_reply": "2022-10-17T08:50:07.466163Z",
     "shell.execute_reply.started": "2022-10-17T08:50:07.292407Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (7, 4)\n",
    "x = np.linspace(5,4000,100)\n",
    "plt.plot(x, RSLE(x,1000))\n",
    "plt.xlabel('prediction')\n",
    "plt.ylabel('RMSLE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
